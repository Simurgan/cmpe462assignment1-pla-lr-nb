{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from logistic_regression import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 3810\n",
      "Number of features: 7\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "rice = fetch_ucirepo(id=545)\n",
    "dataset = rice[\"data\"][\"original\"].to_numpy()\n",
    "\n",
    "num_samples = dataset.shape[0]\n",
    "num_features = dataset.shape[1] - 1\n",
    "num_classes = len(np.unique(dataset[:,-1]))\n",
    "\n",
    "print(f\"Number of samples: {num_samples}\") # 3810\n",
    "print(f\"Number of features: {num_features}\") # 7\n",
    "print(f\"Number of classes: {num_classes}\") # 2\n",
    "\n",
    "mins = [None] * num_features\n",
    "maxes = [None] * num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_feature(data, feature_index, min_val, max_val):\n",
    "    for i in range(len(data)):\n",
    "        if data[i][feature_index] < min_val:\n",
    "            data[i][feature_index] = min_val\n",
    "        elif data[i][feature_index] > max_val:\n",
    "            data[i][feature_index] = max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(data):\n",
    "    for i in range(num_features):\n",
    "        min_val = data.T[i].min()\n",
    "        max_val = data.T[i].max()\n",
    "        for d in range(N):\n",
    "            data[d][i] = (data[d][i] - min_val) / (max_val - min_val)\n",
    "\n",
    "        mins[i] = min_val\n",
    "        maxes[i] = max_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3048, 8)\n",
      "(762, 8)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, ratio):\n",
    "    split = int(ratio * len(data))\n",
    "    return data[:split], data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LogisticRegression.__init__() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m c_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n\u001b[1;32m     47\u001b[0m y \u001b[38;5;241m=\u001b[39m dataset[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 48\u001b[0m optimal_C \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 24\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(X, y, C_values, k)\u001b[0m\n\u001b[1;32m     22\u001b[0m theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# 100 iterations for simplicity\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_train\u001b[38;5;241m.\u001b[39mT, (predictions \u001b[38;5;241m-\u001b[39m y_train)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_train)\n\u001b[1;32m     26\u001b[0m     theta \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m C \u001b[38;5;241m*\u001b[39m gradient\n",
      "\u001b[0;31mTypeError\u001b[0m: LogisticRegression.__init__() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "def k_fold_cross_validation(X, y, C_values, k=5):\n",
    "    fold_size = num_samples // k\n",
    "\n",
    "    # Shuffle indices\n",
    "    indices = np.random.permutation(num_samples)\n",
    "\n",
    "    # Initialize arrays to store scores for each value of C\n",
    "    mean_scores = np.zeros(len(C_values))\n",
    "\n",
    "    for i, C in enumerate(C_values):\n",
    "        scores = []\n",
    "\n",
    "        for j in range(k):\n",
    "            # Split data into training and validation folds\n",
    "            val_indices = indices[j * fold_size: (j + 1) * fold_size]\n",
    "            train_indices = np.concatenate([indices[:j * fold_size], indices[(j + 1) * fold_size:]])\n",
    "\n",
    "            X_train, X_val = X[train_indices], X[val_indices]\n",
    "            y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            # Train logistic regression model\n",
    "            theta = np.zeros(X_train.shape[1])\n",
    "            for _ in range(100):  # 100 iterations for simplicity\n",
    "                predictions = LogisticRegression(X_train, y_train, theta)\n",
    "                gradient = np.dot(X_train.T, (predictions - y_train)) / len(y_train)\n",
    "                theta -= C * gradient\n",
    "\n",
    "            # Predict on validation fold\n",
    "            val_predictions = (LogisticRegression(X_val, y_val, theta) >= 0.5).astype(int)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            score = (np.mean(y_val == val_predictions)) \n",
    "            scores.append(score)\n",
    "\n",
    "        # Calculate mean score for current value of C\n",
    "        mean_scores[i] = np.mean(scores)\n",
    "\n",
    "    # Select optimal value of C\n",
    "    optimal_C_index = np.argmax(mean_scores)\n",
    "    optimal_C = C_values[optimal_C_index]\n",
    "\n",
    "    return optimal_C\n",
    "\n",
    "# Define candidate values for regularization parameter C\n",
    "c_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "y = dataset[:,-1]\n",
    "optimal_C = k_fold_cross_validation(dataset, y, c_values, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
